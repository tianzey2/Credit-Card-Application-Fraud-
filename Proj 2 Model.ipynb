{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X_models,Y_labels,train_size=0.7,test_size = 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_1 = RandomForestClassifier(max_features = 7,\n",
    "                                            n_estimators = 100, max_depth=60)\n",
    "random_forest_1.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_2 = RandomForestClassifier(max_features = 7,\n",
    "                                            n_estimators = 100,max_depth=70)\n",
    "random_forest_2.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_3 = RandomForestClassifier(max_features = 7,\n",
    "                                            n_estimators = 100,max_depth=80)\n",
    "random_forest_3.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_4 = RandomForestClassifier(max_features = 7,\n",
    "                                            n_estimators = 200,max_depth=60)\n",
    "random_forest_4.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_5 = RandomForestClassifier(max_features = 7,\n",
    "                                            n_estimators = 200,max_depth=70)\n",
    "random_forest_5.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_6 = RandomForestClassifier(max_features = 7,\n",
    "                                            n_estimators = 200,max_depth=80)\n",
    "random_forest_6.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_7 = RandomForestClassifier(max_features = 7,\n",
    "                                            n_estimators = 300,max_depth=60)\n",
    "random_forest_7.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_8 = RandomForestClassifier(max_features = 7,\n",
    "                                            n_estimators = 300,max_depth=70)\n",
    "random_forest_8.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_9 = RandomForestClassifier(max_features = 7,\n",
    "                                            n_estimators = 300,max_depth=80)\n",
    "random_forest_9.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdr_RF(model,X_data,Y_data):\n",
    "    Y_data = pd.DataFrame(Y_data)\n",
    "    Y_data['Fraud Proba'] = model.predict_proba(X_data)[:,1].tolist()\n",
    "    Y_data = Y_data.sort_values(by='Fraud Proba',ascending=False)\n",
    "    total_bads = Y_data['fraud_label'][Y_data['fraud_label']==1].count()\n",
    "    top_rows = int(len(X_data)*.03)\n",
    "    sum_bads = Y_data['fraud_label'].head(top_rows)[Y_data['fraud_label']==1].count()\n",
    "    fdr = sum_bads/total_bads\n",
    "    return fdr*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_results = pd.datetime.now()\n",
    "results_dict_RF = {}\n",
    "for i in range(1,10):\n",
    "    curr_time = pd.datetime.now()\n",
    "    model_name = \"random_forest_\"+str(i)\n",
    "    results_dict_RF[model_name] = {'fdr':{}}\n",
    "    \n",
    "    # calculate fdr for training, testing, and validation sets\n",
    "    results_dict_RF[model_name]['fdr']['train_fdr_30']=fdr_RF(vars()[model_name],X_train,Y_train)\n",
    "    results_dict_RF[model_name]['fdr']['test_fdr_30']=fdr_RF(vars()[model_name],X_test,Y_test)\n",
    "    results_dict_RF[model_name]['fdr']['oot_fdr_30']=fdr_RF(vars()[model_name],X_oot,Y_oot)\n",
    "    \n",
    "    print(\"Done with:\",model_name, \";  time:\",pd.datetime.now()-curr_time)\n",
    "    \n",
    "print(\"DONE!\", pd.datetime.now()-time_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Work\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  function to compute FDR \n",
    "def get_FDR(y_scores, y_true):\n",
    "    res_df = pd.DataFrame({'score':y_scores,'label': y_true}).sort_values(by='score',ascending=False)\n",
    "    top3_res1 = res_df.head(round(y_true.shape[0]*0.03))\n",
    "    top3_res2 = res_df.tail(round(y_true.shape[0]*0.03))\n",
    "#     return np.maximum((top3_res1['label'].sum()/sum(y_true)),(top3_res1['label'].sum()/sum(y_true)))\n",
    "    return (top3_res1['label'].sum()/sum(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    y_true = K.ones_like(y_true) \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    all_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    \n",
    "    recall = true_positives / (all_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    y_true = K.ones_like(y_true) \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 30 # number of nodes in hidden layer\n",
    "adam_par = Adam(learning_rate= 0.01, beta_1=0.9, beta_2=0.999, amsgrad=False) # adam algorithm parameters\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(num_hidden, input_dim=X_models.shape[1], activation='relu'))\n",
    "# uncomment to add extra hidden layers\n",
    "# model.add(Dense(20, input_dim=12, activation='relu'))\n",
    "# model.add(Dense(15, input_dim=30, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1_score, precision_m, recall_m, tf.keras.metrics.AUC()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_epochs = 10\n",
    "batch = 1000 # assign batch size in training\n",
    "class_weights = {0: 0.5, 1: 8.5} # assign class weight\n",
    "start_time = pd.datetime.now()\n",
    "history = model.fit(x=X_models, y=Y_labels, batch_size = batch,class_weight = class_weights, epochs = no_of_epochs, validation_split = 0.3)\n",
    "print('duration: ', pd.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas_pred_train = model.predict(X_train) # get class scores\n",
    "clas_pred_test = model.predict(X_test) # get class scores\n",
    "clas_pred_OOT =  model.predict(X_oot) # get class scores\n",
    "print('Train FDR @ 3% is',get_FDR(np.squeeze(clas_pred_train),y_train))\n",
    "print('Test FDR @ 3% is',get_FDR(np.squeeze(clas_pred_test),y_test))\n",
    "print('OOT FDR is:',get_FDR(np.squeeze(clas_pred_OOT),Y_oot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 30 # number of nodes in hidden layer\n",
    "adam_par = Adam(learning_rate= 0.01, beta_1=0.9, beta_2=0.999, amsgrad=False) # adam algorithm parameters\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(num_hidden, input_dim=X_train.shape[1], activation='relu'))\n",
    "# uncomment to add extra hidden layers\n",
    "# model.add(Dense(20, input_dim=12, activation='relu'))\n",
    "# model.add(Dense(15, input_dim=30, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1_score, precision_m, recall_m, tf.keras.metrics.AUC()])\n",
    "no_of_epochs = 20\n",
    "batch = 1000 # assign batch size in training\n",
    "class_weights = {0: 0.5, 1: 8.5} # assign class weight\n",
    "start_time = pd.datetime.now()\n",
    "history = model.fit(x=X_models, y=Y_labels, batch_size = batch,class_weight = class_weights, epochs = no_of_epochs, validation_split = 0.3)\n",
    "print('duration: ', pd.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas_pred_train = model.predict(X_train) # get class scores\n",
    "clas_pred_test = model.predict(X_test) # get class scores\n",
    "clas_pred_OOT =  model.predict(X_oot) # get class scores\n",
    "print('Train FDR @ 3% is',get_FDR(np.squeeze(clas_pred_train),y_train))\n",
    "print('Test FDR @ 3% is',get_FDR(np.squeeze(clas_pred_test),y_test))\n",
    "print('OOT FDR is:',get_FDR(np.squeeze(clas_pred_OOT),Y_oot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 30 # number of nodes in hidden layer\n",
    "adam_par = Adam(learning_rate= 0.01, beta_1=0.9, beta_2=0.999, amsgrad=False) # adam algorithm parameters\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(num_hidden, input_dim=X_train.shape[1], activation='relu'))\n",
    "# uncomment to add extra hidden layers\n",
    "# model.add(Dense(20, input_dim=12, activation='relu'))\n",
    "# model.add(Dense(15, input_dim=30, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1_score, precision_m, recall_m, tf.keras.metrics.AUC()])\n",
    "no_of_epochs = 30\n",
    "batch = 1000 # assign batch size in training\n",
    "class_weights = {0: 0.5, 1: 8.5} # assign class weight\n",
    "start_time = pd.datetime.now()\n",
    "history = model.fit(x=X_models, y=Y_labels, batch_size = batch,class_weight = class_weights, epochs = no_of_epochs, validation_split = 0.3)\n",
    "print('duration: ', pd.datetime.now() - start_time)\n",
    "clas_pred_train = model.predict(X_train) # get class scores\n",
    "clas_pred_test = model.predict(X_test) # get class scores\n",
    "clas_pred_OOT =  model.predict(X_oot) # get class scores\n",
    "print('Train FDR @ 3% is',get_FDR(np.squeeze(clas_pred_train),y_train))\n",
    "print('Test FDR @ 3% is',get_FDR(np.squeeze(clas_pred_test),y_test))\n",
    "print('OOT FDR is:',get_FDR(np.squeeze(clas_pred_OOT),Y_oot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 10 # number of nodes in hidden layer\n",
    "adam_par = Adam(learning_rate= 0.01, beta_1=0.9, beta_2=0.999, amsgrad=False) # adam algorithm parameters\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(num_hidden, input_dim=X_train.shape[1], activation='relu'))\n",
    "# uncomment to add extra hidden layers\n",
    "# model.add(Dense(20, input_dim=12, activation='relu'))\n",
    "# model.add(Dense(15, input_dim=30, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1_score, precision_m, recall_m, tf.keras.metrics.AUC()])\n",
    "no_of_epochs = 10\n",
    "batch = 1000 # assign batch size in training\n",
    "class_weights = {0: 0.5, 1: 8.5} # assign class weight\n",
    "start_time = pd.datetime.now()\n",
    "history = model.fit(x=X_models, y=Y_labels, batch_size = batch,class_weight = class_weights, epochs = no_of_epochs, validation_split = 0.3)\n",
    "print('duration: ', pd.datetime.now() - start_time)\n",
    "clas_pred_train = model.predict(X_train) # get class scores\n",
    "clas_pred_test = model.predict(X_test) # get class scores\n",
    "clas_pred_OOT =  model.predict(X_oot) # get class scores\n",
    "print('Train FDR @ 3% is',get_FDR(np.squeeze(clas_pred_train),y_train))\n",
    "print('Test FDR @ 3% is',get_FDR(np.squeeze(clas_pred_test),y_test))\n",
    "print('OOT FDR is:',get_FDR(np.squeeze(clas_pred_OOT),Y_oot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 10 # number of nodes in hidden layer\n",
    "adam_par = Adam(learning_rate= 0.01, beta_1=0.9, beta_2=0.999, amsgrad=False) # adam algorithm parameters\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(num_hidden, input_dim=X_train.shape[1], activation='relu'))\n",
    "# uncomment to add extra hidden layers\n",
    "# model.add(Dense(20, input_dim=12, activation='relu'))\n",
    "# model.add(Dense(15, input_dim=30, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1_score, precision_m, recall_m, tf.keras.metrics.AUC()])\n",
    "no_of_epochs = 20\n",
    "batch = 1000 # assign batch size in training\n",
    "class_weights = {0: 0.5, 1: 8.5} # assign class weight\n",
    "start_time = pd.datetime.now()\n",
    "history = model.fit(x=X_models, y=Y_labels, batch_size = batch,class_weight = class_weights, epochs = no_of_epochs, validation_split = 0.3)\n",
    "print('duration: ', pd.datetime.now() - start_time)\n",
    "clas_pred_train = model.predict(X_train) # get class scores\n",
    "clas_pred_test = model.predict(X_test) # get class scores\n",
    "clas_pred_OOT =  model.predict(X_oot) # get class scores\n",
    "print('Train FDR @ 3% is',get_FDR(np.squeeze(clas_pred_train),y_train))\n",
    "print('Test FDR @ 3% is',get_FDR(np.squeeze(clas_pred_test),y_test))\n",
    "print('OOT FDR is:',get_FDR(np.squeeze(clas_pred_OOT),Y_oot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 10 # number of nodes in hidden layer\n",
    "adam_par = Adam(learning_rate= 0.01, beta_1=0.9, beta_2=0.999, amsgrad=False) # adam algorithm parameters\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(num_hidden, input_dim=X_train.shape[1], activation='relu'))\n",
    "# uncomment to add extra hidden layers\n",
    "# model.add(Dense(20, input_dim=12, activation='relu'))\n",
    "# model.add(Dense(15, input_dim=30, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1_score, precision_m, recall_m, tf.keras.metrics.AUC()])\n",
    "no_of_epochs = 30\n",
    "batch = 1000 # assign batch size in training\n",
    "class_weights = {0: 0.5, 1: 8.5} # assign class weight\n",
    "start_time = pd.datetime.now()\n",
    "history = model.fit(x=X_models, y=Y_labels, batch_size = batch,class_weight = class_weights, epochs = no_of_epochs, validation_split = 0.3)\n",
    "print('duration: ', pd.datetime.now() - start_time)\n",
    "clas_pred_train = model.predict(X_train) # get class scores\n",
    "clas_pred_test = model.predict(X_test) # get class scores\n",
    "clas_pred_OOT =  model.predict(X_oot) # get class scores\n",
    "print('Train FDR @ 3% is',get_FDR(np.squeeze(clas_pred_train),y_train))\n",
    "print('Test FDR @ 3% is',get_FDR(np.squeeze(clas_pred_test),y_test))\n",
    "print('OOT FDR is:',get_FDR(np.squeeze(clas_pred_OOT),Y_oot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, KFold, StratifiedKFold, cross_val_score, cross_val_predict\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_models, Y_labels, test_size=0.3, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the ratio between the negative and positive classes to use with the scale_pos_weight parameter in XGBoost\n",
    "sum_pos = sum(y_train== 1.0)\n",
    "print(sum_pos)\n",
    "sum_neg = sum(y_train== 0.0)\n",
    "print(sum_neg)\n",
    "ratio = sum_neg / sum_pos\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdr_XGB(data, topRows):\n",
    "    topRows_fs = int(round(len(data)*topRows))\n",
    "    data_topRows = data.head(topRows_fs)\n",
    "    frauds_current = data_topRows.loc[:,'fraud_label']\n",
    "    bads_all = data.loc[data['fraud_label'] == 1]\n",
    "    FDR = sum(frauds_current) / len(bads_all)\n",
    "    return FDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(model, X_data, y_data):\n",
    "    fraud_proba = model.predict_proba(X_data)[:, 1]\n",
    "    curr_data = X_data.copy()\n",
    "    curr_data.insert(0, 'fraud_label', y_data)\n",
    "    curr_data.insert(1, 'fraud_proba', fraud_proba)\n",
    "    curr_data = curr_data.sort_values(['fraud_proba'], ascending=False)\n",
    "    return curr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_xgbfit=pd.datetime.now()\n",
    "xgb_1 =  xgb.XGBClassifier(objective='binary:logistic', learning_rate = 0.01, \n",
    "                           n_estimators=600, max_depth=5, scale_pos_weight=ratio)\n",
    "xgb_1.fit(X_train, y_train)\n",
    "print(\"DONE!\", pd.datetime.now()-start_xgbfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_xgbfit=pd.datetime.now()\n",
    "xgb_2 =  xgb.XGBClassifier(objective='binary:logistic', learning_rate = 0.01, \n",
    "                           n_estimators=800, max_depth=5, scale_pos_weight=ratio)\n",
    "xgb_2.fit(X_train, y_train)\n",
    "print(\"DONE!\", pd.datetime.now()-start_xgbfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_xgbfit=pd.datetime.now()\n",
    "xgb_3 =  xgb.XGBClassifier(objective='binary:logistic', learning_rate = 0.01, \n",
    "                           n_estimators=1000, max_depth=5, scale_pos_weight=ratio)\n",
    "xgb_3.fit(X_train, y_train)\n",
    "print(\"DONE!\", pd.datetime.now()-start_xgbfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_xgbfit=pd.datetime.now()\n",
    "xgb_4 =  xgb.XGBClassifier(objective='binary:logistic', learning_rate = 0.01, \n",
    "                           n_estimators=600, max_depth=4, scale_pos_weight=ratio)\n",
    "xgb_4.fit(X_train, y_train)\n",
    "print(\"DONE!\", pd.datetime.now()-start_xgbfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_xgbfit=pd.datetime.now()\n",
    "xgb_5 =  xgb.XGBClassifier(objective='binary:logistic', learning_rate = 0.01, \n",
    "                           n_estimators=800, max_depth=4, scale_pos_weight=ratio)\n",
    "xgb_5.fit(X_train, y_train)\n",
    "print(\"DONE!\", pd.datetime.now()-start_xgbfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_xgbfit=pd.datetime.now()\n",
    "xgb_6 =  xgb.XGBClassifier(objective='binary:logistic', learning_rate = 0.01, \n",
    "                           n_estimators=1000, max_depth=4, scale_pos_weight=ratio)\n",
    "xgb_6.fit(X_train, y_train)\n",
    "print(\"DONE!\", pd.datetime.now()-start_xgbfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_xgbfit=pd.datetime.now()\n",
    "xgb_7 =  xgb.XGBClassifier(objective='binary:logistic', learning_rate = 0.001, \n",
    "                           n_estimators=600, max_depth=5, scale_pos_weight=ratio)\n",
    "xgb_7.fit(X_train, y_train)\n",
    "print(\"DONE!\", pd.datetime.now()-start_xgbfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_xgbfit=pd.datetime.now()\n",
    "xgb_8 =  xgb.XGBClassifier(objective='binary:logistic', learning_rate = 0.001, \n",
    "                           n_estimators=800, max_depth=5, scale_pos_weight=ratio)\n",
    "xgb_8.fit(X_train, y_train)\n",
    "print(\"DONE!\", pd.datetime.now()-start_xgbfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_xgbfit=pd.datetime.now()\n",
    "xgb_9 =  xgb.XGBClassifier(objective='binary:logistic', learning_rate = 0.001, \n",
    "                           n_estimators=1000, max_depth=5, scale_pos_weight=ratio)\n",
    "xgb_9.fit(X_train, y_train)\n",
    "print(\"DONE!\", pd.datetime.now()-start_xgbfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_xgbfit=pd.datetime.now()\n",
    "xgb_10 =  xgb.XGBClassifier(objective='binary:logistic', learning_rate = 0.001, \n",
    "                           n_estimators=600, max_depth=4, scale_pos_weight=ratio)\n",
    "xgb_10.fit(X_train, y_train)\n",
    "print(\"DONE!\", pd.datetime.now()-start_xgbfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_results=pd.datetime.now()\n",
    "\n",
    "results_dict_XGB={}\n",
    "for num in range(1,17):\n",
    "    curr_time=pd.datetime.now()\n",
    "    curr_model_name = \"xgb_\" + str(num)\n",
    "    \n",
    "    results_dict_XGB[curr_model_name]={'scores':{},\n",
    "                                   'data':{},\n",
    "                                   'FDR':{}\n",
    "                                  }\n",
    "    \n",
    "    # Calculate the accuracy scores of the model \n",
    "    train_score = vars()[curr_model_name].score(X_train, y_train)\n",
    "    test_score = vars()[curr_model_name].score(X_test, y_test)\n",
    "    oot_score = vars()[curr_model_name].score(X_oot, Y_oot)\n",
    "    \n",
    "    # Save the accuracy scores of the model\n",
    "    results_dict_XGB[curr_model_name]['scores']['train_score'] = train_score\n",
    "    results_dict_XGB[curr_model_name]['scores']['test_score'] = test_score\n",
    "    results_dict_XGB[curr_model_name]['scores']['oot_score'] = oot_score\n",
    "    \n",
    "    # Calculate the \".predict_proba\" and make dataframes for all datasets\n",
    "    train_data = make_data(vars()[curr_model_name], X_train, y_train) \n",
    "    test_data = make_data(vars()[curr_model_name], X_test, y_test) \n",
    "    oot_data = make_data(vars()[curr_model_name], X_oot, Y_oot) \n",
    "    \n",
    "    # Save all the dataframes for the model\n",
    "    results_dict_XGB[curr_model_name]['data']['train_data'] = train_data\n",
    "    results_dict_XGB[curr_model_name]['data']['test_data'] = test_data\n",
    "    results_dict_XGB[curr_model_name]['data']['oot_data'] = oot_data\n",
    "    \n",
    "    # Calculate the FDRs\n",
    "    train_FDR = fdr_XGB(train_data, 0.03)\n",
    "    test_FDR = fdr_XGB(test_data, 0.03)\n",
    "    oot_FDR = fdr_XGB(oot_data, 0.03)\n",
    "    \n",
    "    # Save the FDRs\n",
    "    results_dict_XGB[curr_model_name]['FDR']['train_FDR'] = train_FDR\n",
    "    results_dict_XGB[curr_model_name]['FDR']['test_FDR'] = test_FDR\n",
    "    results_dict_XGB[curr_model_name]['FDR']['oot_FDR'] = oot_FDR\n",
    "    \n",
    "    print(\"Done with:\",curr_model_name, \";  time:\",pd.datetime.now()-curr_time)\n",
    "\n",
    "print(\"DONE!\", pd.datetime.now()-time_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in results_dict_XGB.keys():\n",
    "    print(model)\n",
    "    print(results_dict_XGB[model]['FDR'])\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
